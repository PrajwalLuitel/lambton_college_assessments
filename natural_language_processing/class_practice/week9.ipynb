{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Prajwal Luitel (C0927658)  - Natural Language Processing -</center>\n",
    "\n",
    "<center>1st August 2024</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input id: tensor([[  101,  1996,  6614,  2102,  2003,  1037, 10392,  2565,  2012,  1996,\n",
      "         12559,  2669,  2267,   102]])\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "text = \"The AIMT is a fantastic program at the Lambton college\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "print(f\"Input id: {input_ids}\")\n",
    "print(f\"Attention mask: {attention_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word embeddings: torch.Size([1, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    word_embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(f\"Shape of word embeddings: {word_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: the aimt is a fantastic program at the lambton college\n",
      "Tokenized text: ['the', 'aim', '##t', 'is', 'a', 'fantastic', 'program', 'at', 'the', 'lamb', '##ton', 'college']\n",
      "Encoded text: tensor([[  101,  1996,  6614,  2102,  2003,  1037, 10392,  2565,  2012,  1996,\n",
      "         12559,  2669,  2267,   102]])\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(f\"Decoded text: {decoded_text}\")\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(decoded_text)\n",
    "print(f\"Tokenized text: {tokenized_text}\")\n",
    "\n",
    "encoded_text = tokenizer.encode(text, return_tensors='pt')\n",
    "print(f\"Encoded text: {encoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: the\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: aim\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: ##t\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: is\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: a\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: fantastic\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: program\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: at\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: the\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: lamb\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: ##ton\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: college\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token, embedding in zip(tokenized_text, word_embeddings[0]):\n",
    "    print(f\"Token: {token}\")\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embedding: \n",
      "tensor([[-2.0340e-02,  7.2588e-02,  1.7413e-01,  1.0753e-01,  2.9440e-01,\n",
      "         -1.0248e-01,  2.5406e-01,  6.9635e-01, -3.2303e-01, -3.1278e-01,\n",
      "          2.3739e-01, -1.4498e-01,  4.1507e-01,  5.6953e-01,  6.8873e-02,\n",
      "         -1.2940e-01,  3.0408e-01,  3.5122e-02, -2.3468e-01, -1.3765e-01,\n",
      "         -2.0795e-01, -2.2657e-01, -9.1484e-02,  5.3153e-01,  4.5044e-01,\n",
      "          2.0673e-01,  3.3365e-01,  7.0274e-02, -1.5105e-01,  1.1173e-01,\n",
      "          1.2948e-01, -6.0593e-02, -2.1415e-01, -2.6945e-01, -2.0657e-02,\n",
      "          9.4398e-02, -3.6323e-01, -3.4001e-01, -1.7670e-01,  1.1183e-02,\n",
      "         -4.2568e-01, -4.5948e-01, -1.4922e-02,  6.0407e-02, -1.2631e-01,\n",
      "         -3.3636e-01, -1.8430e-01, -1.4505e-01,  1.0150e-01, -2.1398e-01,\n",
      "         -1.5419e-01,  2.0304e-01, -1.7404e-02, -2.4991e-01,  5.6454e-02,\n",
      "          4.8624e-01, -3.8084e-01, -2.0731e-01, -2.0960e-01, -9.5725e-02,\n",
      "         -9.3199e-02,  9.6667e-02, -3.9997e-01,  3.7793e-02, -3.3783e-02,\n",
      "          9.2909e-02,  3.4295e-01,  8.5416e-02, -7.0814e-01, -7.0068e-02,\n",
      "         -2.3115e-01, -3.1544e-01,  2.0631e-01, -1.0437e-01, -2.6437e-01,\n",
      "         -6.0358e-01, -4.0470e-02,  2.9047e-01,  4.9986e-01, -5.5687e-02,\n",
      "          2.9793e-01,  7.2333e-01, -6.5011e-02,  4.3921e-01,  3.2944e-01,\n",
      "          3.5569e-01, -7.5765e-02, -9.2007e-02, -2.0076e-01,  4.8057e-01,\n",
      "         -2.2245e-01, -1.0128e-01, -2.2120e-01,  4.9474e-02,  5.9922e-01,\n",
      "         -2.8150e-01, -5.1715e-02, -3.0772e-01, -1.5185e-01,  9.8903e-02,\n",
      "          1.0267e-02, -4.0794e-01,  1.1819e-01,  4.2909e-01, -8.9982e-02,\n",
      "         -1.4570e-01, -1.6146e-02, -1.1214e-01, -2.5215e-01, -3.2218e-01,\n",
      "         -1.0527e-01, -2.1966e-01,  5.1324e-02, -3.4639e-01, -1.4834e-01,\n",
      "          3.7149e-01,  3.4425e-01, -8.5361e-02, -4.1235e-01, -1.6823e-01,\n",
      "          8.5797e-02,  4.1211e-01,  2.2272e-01,  8.1753e-01, -1.1415e-01,\n",
      "          1.9218e-01, -1.5063e-01, -1.6618e-02, -5.0752e-02, -4.1391e-01,\n",
      "          1.1111e-01,  1.5006e-01,  5.2988e-01, -1.1868e-01,  1.9563e-03,\n",
      "          3.5400e-01, -7.6704e-02, -2.9296e-01, -4.1354e-01,  2.8597e-01,\n",
      "         -1.7765e-01,  3.8040e-02,  1.6324e-01, -3.0581e-01, -2.8192e-03,\n",
      "         -1.4801e-01, -2.1435e-01,  2.9273e-01,  1.2664e-01,  1.7617e-01,\n",
      "          8.4533e-02, -2.0704e-01, -3.9023e-01,  6.0033e-02, -3.8367e-01,\n",
      "         -3.5250e-02, -3.7691e-01,  5.9997e-01,  3.9993e-01,  8.4897e-02,\n",
      "          2.2863e-01,  4.5787e-02, -1.9836e-01, -6.9224e-02, -2.3398e-01,\n",
      "         -3.5098e-02, -1.4817e-01,  1.3264e-01, -2.8493e-02,  2.7991e-01,\n",
      "         -4.6380e-01, -2.1359e-01,  6.3408e-01, -2.2952e-01, -1.4491e-01,\n",
      "         -2.9375e-01,  3.3337e-01, -1.8012e-02, -8.3955e-02,  5.7775e-02,\n",
      "         -9.9067e-01,  4.6049e-01, -8.6219e-02, -3.2826e-01, -2.3641e-01,\n",
      "         -3.9517e-01,  1.1299e-02, -2.6128e-01, -1.4251e-01,  3.0527e-01,\n",
      "          5.3451e-02, -9.3281e-02, -9.0958e-02,  4.1426e-02,  3.3963e-01,\n",
      "         -2.9401e-01, -3.4785e-01,  3.6017e-02, -3.2140e-01,  2.7184e-01,\n",
      "          2.7320e-01, -1.4604e-01, -1.4866e-01, -1.9448e-01, -4.2799e-01,\n",
      "          1.3899e-01,  2.5662e-01,  1.1773e-01,  9.0406e-02,  2.3652e-01,\n",
      "         -6.3460e-03,  2.6719e-01, -1.1098e-02,  2.5366e-01,  2.1884e-02,\n",
      "         -4.4188e-02, -1.6855e-02, -1.9207e-01,  6.1339e-03, -2.7461e-01,\n",
      "          5.5142e-01, -5.6041e-02,  7.5072e-02,  2.8211e-01, -1.0524e-01,\n",
      "          5.4359e-01, -5.6367e-02, -4.1662e-01,  2.1558e-01,  2.3211e-01,\n",
      "          1.4767e-01, -2.5434e-01,  2.6628e-01, -3.4720e-02, -3.1263e-01,\n",
      "          2.4046e-01, -3.8408e-01, -2.2709e-02,  2.2633e-01,  3.7413e-02,\n",
      "          1.6928e-02,  4.2191e-01,  5.7788e-02,  4.9031e-01,  5.5014e-02,\n",
      "         -1.7145e-03,  3.0917e-01,  1.5401e-01, -2.1202e-02, -6.1481e-01,\n",
      "         -3.2752e-01, -5.8794e-01,  9.2626e-02, -8.4469e-02, -2.5008e-01,\n",
      "         -1.8300e-01,  2.3182e-01, -2.6943e-02,  2.5830e-01,  1.2902e-01,\n",
      "          3.4433e-01,  2.3383e-01, -1.7815e-02, -1.3754e-01, -1.3167e-01,\n",
      "         -3.9764e-01,  9.4517e-02,  4.6567e-02,  2.1178e-03,  1.5824e-01,\n",
      "         -2.7927e-01, -7.3416e-02,  2.0992e-01, -2.8649e-01, -3.7095e-01,\n",
      "          2.2870e-02,  1.9889e-01,  1.6371e-01, -2.5864e-01, -2.6855e-01,\n",
      "          4.3259e-02,  2.4943e-01, -4.2842e-01,  3.7434e-02,  7.8385e-02,\n",
      "          2.1135e-01,  1.3451e-01,  1.3583e-01,  5.5554e-02, -4.9578e-02,\n",
      "         -4.9294e-02,  2.4619e-01,  1.3940e-01, -3.1362e-01,  5.4254e-01,\n",
      "         -2.4798e-01,  2.9259e-01,  7.7594e-03,  8.5698e-03,  2.5666e-02,\n",
      "         -9.5485e-02,  1.6065e-01, -3.4027e-03,  2.5010e-01, -5.3425e-02,\n",
      "          4.0227e-01,  1.0473e-01, -2.2170e-01, -4.6914e+00, -4.6169e-01,\n",
      "         -1.0388e-01, -7.9126e-02,  3.2627e-01, -4.2440e-01, -9.5394e-03,\n",
      "         -1.4667e-01, -6.4795e-01, -1.2403e-01, -2.3213e-01, -2.7735e-01,\n",
      "          3.9225e-01,  3.8976e-01,  2.6714e-01,  4.0666e-01, -5.8119e-02,\n",
      "         -1.0451e-01, -8.6975e-02,  5.4607e-01,  2.1624e-01, -3.3230e-01,\n",
      "          4.2684e-02, -2.2399e-01,  1.6536e-01,  1.0117e-01, -5.7059e-01,\n",
      "         -1.0971e-02, -4.4042e-01, -2.9637e-01,  1.3204e-01, -2.6626e-02,\n",
      "          7.5866e-02,  3.6098e-01, -2.3102e-03, -8.6051e-02,  2.7966e-01,\n",
      "         -1.8345e-01, -2.4852e-01, -6.3398e-02,  1.7657e-01, -4.9704e-01,\n",
      "          2.3706e-01,  1.0011e-01,  2.3317e-01, -1.1378e-01,  6.8903e-02,\n",
      "         -2.8631e-01, -6.2757e-02,  3.8813e-03, -2.2422e-01,  3.5725e-02,\n",
      "         -5.8370e-02, -1.5078e-01, -3.8317e-01, -1.9014e-01,  3.6140e-01,\n",
      "          8.1474e-01,  2.5187e-01, -1.8306e-01,  1.5651e-01, -2.5350e-01,\n",
      "          5.2272e-02,  1.5694e-02, -3.0884e-01,  9.3521e-02, -4.4894e-01,\n",
      "         -1.4916e-02,  1.3874e-01,  1.6285e-01,  3.1680e-01, -2.1362e-01,\n",
      "         -4.8325e-01, -1.1545e+00, -1.1540e-01, -3.2018e-01,  1.7189e-01,\n",
      "          2.1550e-01,  2.1525e-01,  5.7970e-03, -3.0903e-01, -4.6190e-01,\n",
      "          4.1799e-01,  2.0116e-01,  3.4702e-01, -3.7662e-01, -4.2483e-02,\n",
      "          3.4617e-02,  6.3631e-02, -2.8385e-01,  1.4669e-01,  1.6179e-01,\n",
      "          9.4735e-02,  3.9802e-01, -9.0260e-02, -1.4785e-01,  2.6670e-02,\n",
      "         -1.4631e-01,  7.3357e-02,  4.5081e-02, -4.2093e-03, -1.5772e-01,\n",
      "          2.4673e-01,  3.2523e-01, -2.7192e-01, -1.3844e-01, -9.8620e-02,\n",
      "          1.6642e-01,  1.4873e-01,  3.1507e-01,  4.1113e-01, -5.4367e-01,\n",
      "         -3.0136e-02,  2.4887e-02,  8.6322e-02,  5.7811e-02, -1.2705e-01,\n",
      "          5.2691e-01, -8.9258e-02,  1.6653e-01,  1.0560e-01,  6.8179e-01,\n",
      "          1.7980e-01,  1.1510e-01, -1.8491e-01, -1.3040e-01, -3.8143e-02,\n",
      "         -2.1545e-01, -4.0781e-01, -1.6556e-01,  9.1070e-02, -3.9832e-01,\n",
      "         -7.0810e-03,  2.1530e-01, -1.2423e-01,  1.6859e-01, -1.8062e-01,\n",
      "          6.7697e-04,  4.1476e-02,  5.8723e-03,  1.7783e-01,  4.3562e-01,\n",
      "          2.4210e-02,  2.2105e-01,  1.4412e-01,  3.3848e-01, -9.5906e-02,\n",
      "          2.4203e-01, -1.8861e-01,  1.9227e-02,  2.2604e-02, -3.4213e-01,\n",
      "          6.0301e-02, -3.4465e-01,  2.8362e-01,  1.0491e-01,  1.6368e-01,\n",
      "         -1.9368e-01, -2.9529e-02,  1.4000e-01, -1.6939e-03,  2.0411e-01,\n",
      "          2.1345e-03,  2.2954e-01,  6.3221e-02,  4.9381e-01,  1.2799e-01,\n",
      "         -2.8341e-01, -3.6484e-01,  1.1189e-01, -1.3903e-02, -1.5875e-01,\n",
      "         -8.4754e-02, -2.7586e-01,  3.6637e-02,  4.2154e-02, -1.4588e-02,\n",
      "         -1.8797e-01, -1.2568e-01,  1.9663e-01,  4.9194e-01, -1.7102e-01,\n",
      "          1.7718e-01,  6.3363e-02,  3.6871e-01,  1.2734e-01, -2.5618e-01,\n",
      "         -2.2996e-01,  3.2687e-01,  1.8025e-01, -1.0808e-01,  3.9317e-01,\n",
      "         -1.4061e-01, -6.3634e-02, -5.7881e-01, -2.8140e-01, -1.8669e-01,\n",
      "         -3.9684e-01,  4.9231e-01,  3.3262e-01,  3.0841e-01,  2.0693e-01,\n",
      "          1.1797e-01,  7.3743e-02, -9.0893e-02,  6.0810e-02,  1.9496e-01,\n",
      "          1.7072e-01,  3.6556e-02, -2.1581e-01, -1.6294e-01, -1.9577e-01,\n",
      "         -8.5885e-02,  4.8311e-01,  4.0983e-02, -1.6218e-01, -2.4915e-02,\n",
      "         -9.1314e-02, -3.3722e-01,  2.6121e-01, -3.8603e-01,  1.2385e-01,\n",
      "          3.9147e-01,  3.5387e-01, -4.9521e-01, -1.2644e-01,  2.8314e-01,\n",
      "         -2.8189e-01, -4.9362e-01, -1.0459e-01,  2.2433e-01, -3.8953e-01,\n",
      "         -2.4094e-01, -1.9822e-02,  7.0421e-02,  1.1614e-01,  2.1212e-01,\n",
      "         -1.2797e-01, -3.6022e-02,  1.2493e-04, -3.2177e-01, -8.3665e-02,\n",
      "         -4.2235e-02, -5.6412e-01, -2.7669e-01, -1.2399e-01,  6.5482e-02,\n",
      "          2.1368e-01, -3.4920e-01,  4.0952e-01, -2.2070e-02, -1.4649e-01,\n",
      "         -8.9716e-02, -2.4855e-01, -1.3302e-01, -1.1765e-01,  2.9043e-01,\n",
      "          1.0800e-01, -2.5639e-01, -1.6597e-01, -7.6107e-02, -2.0115e-01,\n",
      "         -4.9249e-01,  1.2904e-01, -2.1742e-01,  1.2443e-01,  1.7344e-02,\n",
      "          5.7698e-02,  2.2769e-01, -3.1774e-01, -6.9654e-02,  1.9273e-02,\n",
      "         -8.4035e-03, -9.6298e-02,  1.3180e-01, -5.0744e-02,  6.5372e-02,\n",
      "         -5.6488e-01,  1.5122e-01, -1.7503e-01,  2.2696e-01,  2.0890e-01,\n",
      "          4.3065e-02, -6.8537e-02,  4.5089e-01, -2.4361e-02, -4.1860e-01,\n",
      "          2.5571e-01,  5.8900e-02, -9.1859e-02,  8.6884e-02, -3.6867e-01,\n",
      "          4.4962e-02,  9.9050e-02, -2.4436e-01, -3.7045e-02,  4.6289e-01,\n",
      "         -1.8987e-01,  4.1578e-01, -4.8398e-02,  6.0404e-02,  3.0121e-01,\n",
      "          3.2016e-01,  7.6212e-02, -5.0556e-01,  2.4834e-01, -9.3058e-02,\n",
      "         -1.7335e-01,  1.2686e-01,  2.6247e-01,  4.5282e-02, -2.8404e-01,\n",
      "          6.0807e-01,  6.4768e-01, -4.4003e-01,  2.1899e-02, -1.1292e-01,\n",
      "         -2.9162e-01,  3.3873e-01, -2.7183e-01, -2.2455e-01,  5.3191e-02,\n",
      "         -5.5749e-03,  3.5437e-01,  2.4192e-01,  5.1298e-01, -2.2518e-01,\n",
      "         -4.6568e-01,  1.7726e-01,  3.4386e-01, -1.9199e-01,  2.8494e-01,\n",
      "         -2.4871e-01,  4.2839e-01, -2.3280e-01, -7.5327e-02,  2.5384e-01,\n",
      "         -1.3331e-01, -1.2483e-01,  1.9599e-02, -6.0583e-02,  1.0025e-01,\n",
      "          4.7789e-01,  4.5847e-01,  2.2146e-01,  1.4623e-01,  3.5756e-01,\n",
      "          1.7370e-01,  1.1212e-01,  2.6369e-01, -7.8834e-02,  9.7910e-02,\n",
      "          6.4429e-02, -2.5032e-01,  1.8587e-01,  2.7935e-01, -8.5739e-02,\n",
      "         -1.7459e-02, -1.4815e-01,  6.1911e-02,  2.5448e-01,  8.6987e-02,\n",
      "          2.4995e-01, -4.2663e-01,  6.7829e-02, -2.2011e-01,  4.6205e-01,\n",
      "         -2.5948e-01, -3.3406e-01,  1.4797e-01, -1.1176e-02,  9.6262e-02,\n",
      "          1.7214e-01, -1.6436e-01,  1.4990e-01,  4.2438e-01,  3.2503e-01,\n",
      "         -1.4404e-01,  2.7154e-02,  1.8341e-01, -1.8059e-02, -1.6036e-01,\n",
      "         -4.1005e-01,  1.2388e-01, -2.7903e-01, -5.7782e-01,  2.1581e-05,\n",
      "          1.3508e-01, -4.5153e-01, -8.6764e-03, -9.2136e-03, -1.0400e-01,\n",
      "          3.8758e-01, -1.8271e-02,  3.0967e-01, -2.7539e-01,  1.5761e-01,\n",
      "         -1.2230e-01,  4.4452e-02,  2.0722e-01,  1.6857e-01, -5.9322e-02,\n",
      "         -4.3515e-01, -1.6156e-01, -3.0587e-01, -1.4053e-01,  2.4918e-01,\n",
      "          7.1672e-02, -2.1368e-01, -3.2987e-01,  1.2332e-01,  4.7016e-01,\n",
      "         -5.4932e-01, -2.4688e-02, -2.6476e-02,  4.0064e-01,  1.4870e-01,\n",
      "          1.5042e-01, -2.2568e-01,  1.3574e-01, -4.5266e-01, -2.2016e-01,\n",
      "         -5.0700e-02,  3.2668e-01,  1.6113e-01,  2.3183e-01, -2.9977e-01,\n",
      "          4.6250e-01, -2.5706e-01, -5.4896e-02,  3.9348e-01,  6.0602e-02,\n",
      "          3.6632e-02,  4.1111e-01,  1.2469e-01, -4.9239e-01, -5.4420e-01,\n",
      "          2.6522e-01, -4.7403e-02, -6.2553e-02,  1.7109e-02, -5.9203e-02,\n",
      "          8.0005e-02, -2.1951e-01, -3.9584e-01, -2.4653e-01,  2.1671e-02,\n",
      "         -1.9516e-01, -2.8393e-01, -3.9387e-01, -2.9304e-02, -2.7923e-01,\n",
      "         -9.9732e-02, -2.8972e-01, -2.5446e-01, -2.5492e-01,  1.7096e-01,\n",
      "         -3.1274e-02,  2.6364e-01, -4.2201e-02]])\n",
      "Shape of Sentence Embedding: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding = word_embeddings.mean(dim=1)\n",
    "\n",
    "print(\"Sentence Embedding: \")\n",
    "print(sentence_embedding)\n",
    "\n",
    "print(f\"Shape of Sentence Embedding: {sentence_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity score: 0.8371533155441284\n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"The Lambton college is a great place and AIMT program opens the door to future opportunities\"\n",
    "\n",
    "example_encoding = tokenizer(example_sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "example_input_ids = example_encoding['input_ids']\n",
    "example_attention_mask = example_encoding['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "    example_outputs = model(example_input_ids, attention_mask=example_attention_mask)\n",
    "    example_sentence_embedding = example_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "similarity_score = cosine_similarity(sentence_embedding, example_sentence_embedding)\n",
    "print(f\"Cosine similarity score: {similarity_score[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
